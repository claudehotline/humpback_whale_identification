{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TripletMarginLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize, LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "embedding_dim = 250\n",
    "batch_size = 32\n",
    "\n",
    "data_root = '../../data/humpback_whale_prediction/'\n",
    "train_csv = 'train.csv'\n",
    "train_triplet_file = 'train_triplets.txt'\n",
    "valid_triplet_file = 'valid_triplets.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_root, df, triplet_file_path, triplet_size=5000, transform=None):\n",
    "        '''\n",
    "            data_root: The root path of the data folder.\n",
    "            df: Dataframe of the dataset\n",
    "            transform: the transforms to preprocess the input dataset\n",
    "        '''\n",
    "        super(TripletsDataset, self).__init__()\n",
    "        \n",
    "        self.df = df \n",
    "        self.triplet_size = triplet_size\n",
    "        self.triplet_file_path = triplet_file_path\n",
    "        \n",
    "        # 1. Get image name and corresponding labels\n",
    "        self.image_names, self.image_labels = self.df.Image.values, self.df.Id.values\n",
    "\n",
    "        self.label_to_imagenames = defaultdict(list)       \n",
    "        for image_name, label in zip(list(self.image_names), list(self.image_labels)):\n",
    "            #if id_ == other_id:\n",
    "            #    self.new_whale_list.append(imagename)\n",
    "            #else:\n",
    "            self.label_to_imagenames[label].append(image_name)\n",
    "        \n",
    "        \n",
    "        # 2. Make triplets list\n",
    "        self.triplets = []\n",
    "        \n",
    "        self.make_triplet_list()\n",
    "        for line in open(os.path.join(data_root, self.triplet_file_path)):\n",
    "            img1, img2, img3 = line.split()\n",
    "            self.triplets.append([img1, img2, img3])\n",
    "        \n",
    "        # 3. Set data transform\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img1, img2, img3 = self.triplets[index][0], self.triplets[index][1], self.triplets[index][2]\n",
    "        \n",
    "        img1 = Image.open(os.path.join(data_root, 'train/'+img1)).convert('RGB')\n",
    "        img2 = Image.open(os.path.join(data_root, 'train/'+img2)).convert('RGB')\n",
    "        img3 = Image.open(os.path.join(data_root, 'train/'+img3)).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "            img3 = self.transform(img3)\n",
    "        \n",
    "        return img1, img2, img3\n",
    "    \n",
    "    def make_triplet_list(self):\n",
    "        \n",
    "        print('Generating triplets...')\n",
    "        \n",
    "        triplets = []\n",
    "        \n",
    "        triplet_count = 0\n",
    "        \n",
    "        #for i in range(triplet_size):\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            anchor = None\n",
    "            positive = None\n",
    "            \n",
    "            label = np.random.choice(self.image_labels, size=1)[0]\n",
    "            \n",
    "            if len(self.label_to_imagenames[label]) > 1:\n",
    "                positives = np.random.choice((self.label_to_imagenames[label]), size=2, replace=False)\n",
    "                anchor = positives[0]\n",
    "                positive = positives[1]\n",
    "            \n",
    "                negative = None\n",
    "                while negative == None or self.df[self.df['Image'] == negative]['Id'].values == label:\n",
    "                    negative = np.random.choice(self.df['Image'], 1)[0]\n",
    "            \n",
    "                triplets.append([anchor, positive, negative])\n",
    "                \n",
    "                triplet_count += 1\n",
    "                \n",
    "            if triplet_count >= self.triplet_size:\n",
    "                break\n",
    "                \n",
    "        with open(os.path.join(data_root, self.triplet_file_path), \"w\") as f:\n",
    "            writer = csv.writer(f, delimiter=' ')\n",
    "            writer.writerows(triplets)\n",
    "        print('Done!')\n",
    "        \n",
    "        \n",
    "    def prepare_labels(self, y):\n",
    "        values = np.array(y)\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(values)\n",
    "        \n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "        \n",
    "        y = onehot_encoded\n",
    "        \n",
    "        return y, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize((image_size, image_size)),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                       std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "valid_transform = transforms.Compose([transforms.Resize((image_size, image_size)),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                       std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating triplets...\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(os.path.join(data_root, train_csv))\n",
    "train_df,  valid_df = train_test_split(data_df, train_size=0.7, test_size=0.3, random_state=43)\n",
    "\n",
    "train_dataset = TripletsDataset(data_root, train_df, train_triplet_file, 50000, train_transform)\n",
    "valid_dataset = TripletsDataset(data_root, valid_df, valid_triplet_file, 10000, valid_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1, input2, input3 = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1.shape, input2.shape, input3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the triplet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TripletModel, self).__init__()\n",
    "        self.base_model = models.resnet50(pretrained=True)\n",
    "        \n",
    "        #for param in self.base_model.parameters():\n",
    "        #    param.requires_grad = False\n",
    "        \n",
    "        self.base_model.fc = nn.Linear(2048, embedding_dim)\n",
    "        \n",
    "    def forward(self, x, y, z):\n",
    "        x = self.base_model(x)\n",
    "        y = self.base_model(y)\n",
    "        z = self.base_model(z)\n",
    "        \n",
    "        return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_model = TripletModel()\n",
    "\n",
    "#criterion = torch.nn.MarginRankingLoss(margin = 0.2)\n",
    "optimizer = optim.SGD(trip_model.parameters(), lr=0.001, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, optimizer, num_epochs=25):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    since = time.time()\n",
    "    \n",
    "    val_loss_history = []\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 999999\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'valid']:\n",
    "            \n",
    "            steps = 0\n",
    "            \n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            \n",
    "            for inputs in tqdm(dataloaders[phase]):\n",
    "                inputs[0] = inputs[0].to(device)\n",
    "                inputs[1] = inputs[1].to(device)\n",
    "                inputs[2] = inputs[2].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    x, y, z = model(*inputs)\n",
    "                    \n",
    "                    trip_loss = TripletMarginLoss()\n",
    "                    loss = trip_loss(x, y, z)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # statistics      \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                steps += 1\n",
    "            \n",
    "            epoch_loss = running_loss / steps\n",
    "            \n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'valid':\n",
    "                val_loss_history.append(best_loss)\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    \n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Loss: {:4f}'.format(best_loss))\n",
    "    \n",
    "    # load best model \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataloaders = {'train': train_dataloader, 'valid': valid_dataloader}\n",
    "\n",
    "train_model(trip_model, dataloaders, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
